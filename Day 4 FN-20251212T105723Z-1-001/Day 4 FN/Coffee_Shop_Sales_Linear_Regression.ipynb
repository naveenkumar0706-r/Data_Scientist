{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a5b5012",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.feature_selection import SelectKBest, f_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ee5ab43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5095798e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Sridevi\\\\OneDrive\\\\for online\\\\algorithms\\\\coffee_shop_sales_dataset.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load the main dataset\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mr\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mC:\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mUsers\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mSridevi\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mOneDrive\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mfor online\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43malgorithms\u001b[39;49m\u001b[33;43m\\\u001b[39;49m\u001b[33;43mcoffee_shop_sales_dataset.xlsx\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mDaily_Sales_Data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úì Dataset loaded successfully!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mDataset shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdf.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:495\u001b[39m, in \u001b[36mread_excel\u001b[39m\u001b[34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend, engine_kwargs)\u001b[39m\n\u001b[32m    493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[32m    494\u001b[39m     should_close = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m     io = \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine != io.engine:\n\u001b[32m    502\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    503\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mEngine should not be specified when passing \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    504\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    505\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1550\u001b[39m, in \u001b[36mExcelFile.__init__\u001b[39m\u001b[34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[39m\n\u001b[32m   1548\u001b[39m     ext = \u001b[33m\"\u001b[39m\u001b[33mxls\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     ext = \u001b[43minspect_excel_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\n\u001b[32m   1552\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1553\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ext \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1554\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1555\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mExcel file format cannot be determined, you must specify \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1556\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33man engine manually.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1557\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1402\u001b[39m, in \u001b[36minspect_excel_format\u001b[39m\u001b[34m(content_or_path, storage_options)\u001b[39m\n\u001b[32m   1399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(content_or_path, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[32m   1400\u001b[39m     content_or_path = BytesIO(content_or_path)\n\u001b[32m-> \u001b[39m\u001b[32m1402\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1403\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcontent_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m   1404\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m   1405\u001b[39m     stream = handle.handle\n\u001b[32m   1406\u001b[39m     stream.seek(\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\navee\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:882\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    873\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(\n\u001b[32m    874\u001b[39m             handle,\n\u001b[32m    875\u001b[39m             ioargs.mode,\n\u001b[32m   (...)\u001b[39m\u001b[32m    878\u001b[39m             newline=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    879\u001b[39m         )\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m882\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    883\u001b[39m     handles.append(handle)\n\u001b[32m    885\u001b[39m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Sridevi\\\\OneDrive\\\\for online\\\\algorithms\\\\coffee_shop_sales_dataset.xlsx'"
     ]
    }
   ],
   "source": [
    "# Load the main dataset\n",
    "df = pd.read_excel(r'C:\\Users\\Sridevi\\OneDrive\\for online\\algorithms\\coffee_shop_sales_dataset.xlsx', sheet_name='Daily_Sales_Data')\n",
    "print(f\"‚úì Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31126f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info\n",
    "print(\"\\nüìä Dataset Overview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f34bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìà Dataset Info:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044f0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for missing values\n",
    "print(f\"\\nüîç Missing values: {df.isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9565633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c48ce3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create additional time-based features\n",
    "df['Day_of_Year'] = df['Date'].dt.dayofyear\n",
    "df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
    "df['Quarter'] = df['Date'].dt.quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e35a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a313c8",
   "metadata": {},
   "source": [
    "[red, green, blue, green, red] --> [2,1,0,1,2]  -->[blue, green, red]  -- blue -0, green - 1, red - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7164bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables   #fit(), transform() fit_transform()\n",
    "le = LabelEncoder()\n",
    "df['Day_Name_Encoded'] = le.fit_transform(df['Day_Name'])\n",
    "df['Season_Encoded'] = le.fit_transform(df['Season'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c4d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5ede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling (exclude target and non-predictive columns)\n",
    "exclude_cols = ['Date', 'Day_Name', 'Season', 'Daily_Revenue', 'Staff_Cost', \n",
    "                'Ingredient_Cost', 'Utilities_Cost', 'Rent_Cost', 'Total_Costs', \n",
    "                'Daily_Profit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f0a4811",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "X = df[feature_cols]\n",
    "y = df['Daily_Revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c7b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"‚úì Features selected: {len(feature_cols)} columns\")\n",
    "print(f\"‚úì Target variable: Daily_Revenue\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c62e309",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b6405d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature list\n",
    "print(\"\\nüìã Features used in model:\")\n",
    "for i, col in enumerate(feature_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7286c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\nTarget Variable (Daily_Revenue) Statistics:\")\n",
    "print(f\"Mean: ${y.mean():.2f}\")\n",
    "print(f\"Median: ${y.median():.2f}\")\n",
    "print(f\"Std Dev: ${y.std():.2f}\")\n",
    "print(f\"Min: ${y.min():.2f}\")\n",
    "print(f\"Max: ${y.max():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f7e81ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "print(\"\\nüîó Top 10 Features Correlated with Daily Revenue:\")\n",
    "correlations = df[feature_cols + ['Daily_Revenue']].corr()['Daily_Revenue'].sort_values(ascending=False)\n",
    "print(correlations.head(11)[1:])  # Exclude self-correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263d2f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Coffee Shop Sales Data Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "\"\"\"plt.subplots(2, 3)\n",
    "‚ûî Creates a grid of 2 rows and 3 columns of subplots (total 6 plots).\n",
    "‚ûî axes is a 2D array of axes objects for each subplot.\n",
    "\n",
    "figsize=(18, 12)\n",
    "‚ûî Size of the overall figure (18 inches wide x 12 inches tall).\n",
    "\n",
    "fig.suptitle()\n",
    "‚ûî Adds a big main title on top of the entire figure.\n",
    "\"\"\"\n",
    "\n",
    "# Revenue distribution\n",
    "axes[0, 0].hist(y, bins=30, alpha=0.7, color='brown', edgecolor='black')\n",
    "axes[0, 0].set_title('Daily Revenue Distribution')\n",
    "axes[0, 0].set_xlabel('Revenue ($)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "\"\"\"axes[0, 0]\n",
    "‚ûî Refers to the subplot at first row, first column.\n",
    "\n",
    ".hist(y, bins=30)\n",
    "‚ûî Plots a histogram of the Daily Revenue with 30 bins.\n",
    "\n",
    "alpha=0.7\n",
    "‚ûî Transparency of the histogram bars.\n",
    "\n",
    "color='brown', edgecolor='black'\n",
    "‚ûî Brown bars with black edges for better visibility.\n",
    "\n",
    "grid(True, alpha=0.3)\n",
    "‚ûî Adds gridlines to the plot with light transparency.\"\"\"\n",
    "\n",
    "# Top correlations\n",
    "top_features = correlations.head(6)[1:].index  # Top 5 excluding self\n",
    "for i, feature in enumerate(top_features):\n",
    "    if i < 5:\n",
    "        row = i // 3\n",
    "        col = (i + 1) % 3\n",
    "        if row == 0 and col == 0:\n",
    "            continue  # Skip first position (already used)\n",
    "        axes[row, col].scatter(df[feature], y, alpha=0.6)\n",
    "        axes[row, col].set_title(f'{feature} vs Revenue\\n(r = {correlations[feature]:.3f})')\n",
    "        axes[row, col].set_xlabel(feature)\n",
    "        axes[row, col].set_ylabel('Daily Revenue ($)')\n",
    "        axes[row, col].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\"\"\"You're getting the top 5 features most correlated with Daily Revenue.\n",
    "\n",
    ".head(6) picks top 6 (including the revenue itself), then [1:] skips the first (self-correlation).\n",
    "enumerate(top_features)\n",
    "‚ûî Loop over the features with their index.\n",
    "\n",
    "if i < 5:\n",
    "‚ûî Only do this for the top 5 features.\n",
    "\n",
    "row = i // 3 and col = (i + 1) % 3\n",
    "‚ûî Smart way to position the scatter plots in the 2√ó3 grid without overlapping the histogram at (0, 0).\n",
    "\n",
    "if row == 0 and col == 0:\n",
    "‚ûî Skip the position (0, 0) since it's already used by histogram.\n",
    "\n",
    "scatter(df[feature], y, alpha=0.6)\n",
    "‚ûî Scatter plot of feature vs Daily Revenue with some transparency.\n",
    "\n",
    ".set_title()\n",
    "‚ûî Title includes correlation coefficient (r value) with revenue.\n",
    "\n",
    "plt.tight_layout()\n",
    "‚ûî Adjusts spacing so labels and titles don't overlap.\n",
    "\n",
    "plt.show()\n",
    "‚ûî Displays the figure.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "# Select subset of most important features for readability\n",
    "important_features = correlations.head(11)[1:].index[:10]\n",
    "print(f\"\\nüîó Top 10 Features for Heatmap: {list(important_features)}\")\n",
    "corr_matrix = df[list(important_features) + ['Daily_Revenue']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'label': 'Correlation'})\n",
    "plt.title('Feature Correlation Matrix (Top 10 Features)', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de99641e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"\\nüìä Data Split:\")\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68d02cf",
   "metadata": {},
   "source": [
    "üßÆ What is Feature Scaling?\n",
    "\n",
    "Feature scaling means bringing all input features to the same scale, usually with mean = 0 and standard deviation = 1.\n",
    "\n",
    "For example:\n",
    "\n",
    "- Some features may be in lakhs (‚Çπ) like revenue\n",
    "\n",
    "- Some may be in 0‚Äì10 scale like satisfaction score\n",
    "\n",
    "- Some may be binary (0/1)\n",
    "\n",
    "If we don‚Äôt scale, features with larger ranges can dominate the model's learning process.\n",
    "\n",
    "üìè Why StandardScaler?\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "StandardScaler subtracts the mean and divides by the standard deviation for each feature.\n",
    "\n",
    "After scaling:\n",
    "\n",
    "Mean = 0\n",
    "\n",
    "Standard Deviation = 1\n",
    "\n",
    "This is especially important for regression models, gradient descent algorithms, and distance-based models.\n",
    "\n",
    "üèãÔ∏è Fit and Transform the Training Data\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    ".fit() learns the mean and standard deviation from X_train.\n",
    "\n",
    ".transform() uses those values to scale X_train data.\n",
    "\n",
    "‚ö†Ô∏è Important:\n",
    "\n",
    "We always fit only on the training set to prevent data leakage from the test set.\n",
    "\n",
    "üõ†Ô∏è Transform the Test Data Using the Same Scaler\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "Now, we only transform X_test using the mean and std learned from training data.\n",
    "\n",
    "Why?\n",
    "\n",
    "We want the model to see test data under the same conditions as training data ‚Äî this ensures fair evaluation.\n",
    "\n",
    "‚úÖ Why are we using it here?\n",
    "\n",
    "- Linear Regression works better and converges faster when features are scaled.\n",
    "\n",
    "- Helps avoid issues caused by large variations in data range.\n",
    "\n",
    "- Makes coefficients easier to interpret.\n",
    "\n",
    "- Prepares the data before applying feature selection and model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5426ad8",
   "metadata": {},
   "source": [
    "\n",
    "z = (x - mean)/SD   -- mean = 20, SD = 13   --> (50 - 20)/13\n",
    "\n",
    "[[50], [60]] -->  [[-1.414], [-0.707]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a25baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c9cbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46ab32e",
   "metadata": {},
   "source": [
    "üéØ Feature Selection using SelectKBest\n",
    "\n",
    "After scaling, we select the most relevant features to improve model performance.\n",
    "\n",
    "This helps reduce noise, avoid overfitting, and make the model faster.\n",
    "\n",
    "‚úÖ Step-by-Step Explanation:\n",
    "\n",
    "selector = SelectKBest(score_func=f_regression, k=15)\n",
    "\n",
    "SelectKBest: A feature selection method from sklearn that selects the top K features based on statistical tests.\n",
    "\n",
    "f_regression: The scoring function that measures the linear correlation between each feature and the target variable (y_train).\n",
    "\n",
    "k=15: We are selecting the top 15 features with the highest correlation (you can adjust this number).\n",
    "\n",
    "\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "\n",
    "fit_transform() on Training Data:\n",
    "\n",
    "fit() calculates the F-statistic and p-values between each feature in X_train_scaled and the target y_train.\n",
    "\n",
    "transform() selects the top 15 features and removes the rest.\n",
    "\n",
    "This reduces the number of input features for the model.\n",
    "\n",
    "\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "Apply transform() only on X_test_scaled.\n",
    "\n",
    "We don't fit() on test data to avoid data leakage.\n",
    "\n",
    "This ensures we evaluate our model on unseen data with the same selected features.\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "\n",
    "get_support() returns a boolean mask of selected features.\n",
    "\n",
    "We use it to get the actual names of the selected features for display or interpretation.\n",
    "\n",
    "\n",
    "print(f\"\\nüéØ Feature Selection: Top {len(selected_features)} features selected\")\n",
    "\n",
    "print(\"Selected features:\", list(selected_features))\n",
    "\n",
    "Prints the number of selected features and their names.\n",
    "\n",
    "Helps us understand which features are contributing to the model.\n",
    "\n",
    "üìù In Simple Words:\n",
    "\n",
    "We select the top 15 most useful features based on how strongly they are related to the target (Daily Revenue).\n",
    "\n",
    "This simplifies the model, makes it faster, and may improve prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3dcecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"‚úì Features scaled using StandardScaler\")\n",
    "\n",
    "# Feature selection (optional - select top K features)\n",
    "selector = SelectKBest(score_func=f_regression, k=15)  # Select top 15 features\n",
    "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
    "X_test_selected = selector.transform(X_test_scaled)\n",
    "\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(f\"\\nüéØ Feature Selection: Top {len(selected_features)} features selected\")\n",
    "print(\"Selected features:\", list(selected_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68503519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the linear regression model\n",
    "print(\"\\nü§ñ Training Linear Regression Model:\")\n",
    "model = LinearRegression()\n",
    "model.fit(X_train_selected, y_train)\n",
    "print(\"‚úì Model trained successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ae77b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "y_pred_train = model.predict(X_train_selected)\n",
    "y_pred_test = model.predict(X_test_selected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31592d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate comprehensive metrics\n",
    "def calculate_metrics(y_true, y_pred, dataset_name):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    # Additional metrics\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "    \n",
    "    print(f\"\\nüìä {dataset_name} Set Performance:\")\n",
    "    print(f\"R¬≤ Score: {r2:.4f}\")\n",
    "    print(f\"RMSE: ${rmse:.2f}\")\n",
    "    print(f\"MAE: ${mae:.2f}\")\n",
    "    print(f\"MAPE: {mape:.2f}%\")\n",
    "    \n",
    "    return {'R2': r2, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53fea43",
   "metadata": {},
   "source": [
    "What it does:\n",
    "\n",
    "MAPE measures the average percentage error between the actual values (y_true) and the predicted values (y_pred).\n",
    "\n",
    "How it's calculated:\n",
    "\n",
    "- (y_true - y_pred) ‚Üí Difference (error) between actual and predicted values.\n",
    "\n",
    "- np.abs(...) ‚Üí Take absolute value so negative errors don‚Äôt cancel out positive ones.\n",
    "\n",
    "- ... / y_true ‚Üí Divide by actual values to convert to a percentage error.\n",
    "\n",
    "- np.mean(...) ‚Üí Average across all samples.\n",
    "\n",
    "- * 100 ‚Üí Convert it into a percentage.\n",
    "\n",
    "Why use it:\n",
    "\n",
    "MAPE is useful when you want to express prediction error as a percentage ‚Äî easy for business interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6760ac67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model performance\n",
    "train_metrics = calculate_metrics(y_train, y_pred_train, \"Training\")\n",
    "test_metrics = calculate_metrics(y_test, y_pred_test, \"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a067cf",
   "metadata": {},
   "source": [
    "### Training set performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0019a81",
   "metadata": {},
   "source": [
    "| Metric               | Value                                                                                                                                                                         | What it Means |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |\n",
    "| **R¬≤ Score: 0.9573** | The model explains **95.73%** of the variance in the training data. This is a high score, meaning the model fits the training data very well.                                 |               |\n",
    "| **RMSE: \\$21.05**    | On average, the predicted revenue is **\\$21.05 off** from the actual value. RMSE penalizes larger errors more than smaller ones.                                              |               |\n",
    "| **MAE: \\$16.15**     | The model‚Äôs prediction is **\\$16.15 off on average** (without squaring errors). Easier to interpret than RMSE.                                                                |               |\n",
    "| **MAPE: 5.63%**      | On average, the model‚Äôs predictions are **5.63% away from the actual values** in percentage terms. This is a low percentage error, indicating high accuracy on training data. |               |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d9b2ce",
   "metadata": {},
   "source": [
    "### Test set performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f28bcb",
   "metadata": {},
   "source": [
    "| Metric               | Value                                                                                                                   | What it Means |\n",
    "| -------------------- | ----------------------------------------------------------------------------------------------------------------------- | ------------- |\n",
    "| **R¬≤ Score: 0.9465** | The model explains **94.65%** of the variance in unseen test data. Very good generalization.                            |               |\n",
    "| **RMSE: \\$25.20**    | On unseen data, predictions are **\\$25.20 off on average** (slightly higher error than training, which is expected).    |               |\n",
    "| **MAE: \\$19.75**     | Average absolute error on test data is **\\$19.75**, slightly higher than on training.                                   |               |\n",
    "| **MAPE: 9.59%**      | Predictions on test data are, on average, **9.59% away from actual values**. Still acceptable for business forecasting. |               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e961dc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for overfitting\n",
    "print(f\"\\nüîç Overfitting Check:\")\n",
    "print(f\"R¬≤ difference (Train - Test): {train_metrics['R2'] - test_metrics['R2']:.4f}\")\n",
    "if abs(train_metrics['R2'] - test_metrics['R2']) < 0.05:\n",
    "    print(\"‚úì Model appears to generalize well (low overfitting)\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Potential overfitting detected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0904b8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': selected_features,\n",
    "    'Coefficient': model.coef_,\n",
    "    'Abs_Coefficient': np.abs(model.coef_)\n",
    "}).sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "print(f\"\\nüéØ Feature Importance (Top 10):\")\n",
    "print(feature_importance.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db052e",
   "metadata": {},
   "source": [
    "We want to understand how the model makes predictions by:\n",
    "\n",
    "* Seeing the model‚Äôs intercept (baseline value)\n",
    "\n",
    "* Identifying which features influence the revenue the most\n",
    "\n",
    "* Knowing whether each feature increases or decreases revenue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13673b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model interpretation\n",
    "print(f\"\\nüìã Model Interpretation:\")\n",
    "print(f\"Intercept: ${model.intercept_:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4d6da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Linear Regression Model Results', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Actual vs Predicted (Training)\n",
    "axes[0, 0].scatter(y_train, y_pred_train, alpha=0.6, color='blue', s=30)\n",
    "axes[0, 0].plot([y_train.min(), y_train.max()], [y_train.min(), y_train.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Revenue ($)')\n",
    "axes[0, 0].set_ylabel('Predicted Revenue ($)')\n",
    "axes[0, 0].set_title(f'Training Set: Actual vs Predicted\\n(R¬≤ = {train_metrics[\"R2\"]:.3f})')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Actual vs Predicted (Testing)\n",
    "axes[0, 1].scatter(y_test, y_pred_test, alpha=0.6, color='green', s=30)\n",
    "axes[0, 1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 1].set_xlabel('Actual Revenue ($)')\n",
    "axes[0, 1].set_ylabel('Predicted Revenue ($)')\n",
    "axes[0, 1].set_title(f'Test Set: Actual vs Predicted\\n(R¬≤ = {test_metrics[\"R2\"]:.3f})')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Residuals plot\n",
    "residuals_test = y_test - y_pred_test\n",
    "axes[1, 0].scatter(y_pred_test, residuals_test, alpha=0.6, color='red', s=30)\n",
    "axes[1, 0].axhline(y=0, color='black', linestyle='--', alpha=0.8)\n",
    "axes[1, 0].set_xlabel('Predicted Revenue ($)')\n",
    "axes[1, 0].set_ylabel('Residuals ($)')\n",
    "axes[1, 0].set_title('Residuals Plot (Test Set)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Feature importance\n",
    "top_features_plot = feature_importance.head(10)\n",
    "axes[1, 1].barh(range(len(top_features_plot)), top_features_plot['Coefficient'])\n",
    "axes[1, 1].set_yticks(range(len(top_features_plot)))\n",
    "axes[1, 1].set_yticklabels(top_features_plot['Feature'])\n",
    "axes[1, 1].set_xlabel('Coefficient Value')\n",
    "axes[1, 1].set_title('Top 10 Feature Coefficients')\n",
    "axes[1, 1].axvline(x=0, color='black', linestyle='-', alpha=0.3)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe46391a",
   "metadata": {},
   "source": [
    "## Insights from Each Plot\n",
    "\n",
    "### üîπ Top Left: Training Set ‚Äì Actual vs Predicted\n",
    "\n",
    "üìä Insight:\n",
    "\n",
    "The points are very close to the red dashed line (y = x), showing a strong fit to the training data.\n",
    "\n",
    "R¬≤ = 0.957 means that 95.7% of the variation in actual revenue is explained by the model.\n",
    "\n",
    "The model learned well from the training data with very low error.\n",
    "\n",
    "‚úÖ Conclusion: The model fits the training data very well.\n",
    "\n",
    "### üîπ Top Right: Test Set ‚Äì Actual vs Predicted\n",
    "\n",
    "üìä Insight:\n",
    "\n",
    "Test data points also lie close to the red dashed line, indicating good generalization.\n",
    "\n",
    "R¬≤ = 0.947 shows very good predictive performance on unseen data.\n",
    "\n",
    "No major overfitting is observed (since train R¬≤ ‚âà test R¬≤).\n",
    "\n",
    "‚úÖ Conclusion: The model is reliable and generalizes well to new data.\n",
    "\n",
    "### üîπ Bottom Left: Residuals Plot (Test Set)\n",
    "\n",
    "üìä Insight:\n",
    "\n",
    "Residuals are scattered randomly around zero, which is good.\n",
    "\n",
    "There‚Äôs no obvious pattern or curve, meaning the model doesn't miss any important non-linear relationship.\n",
    "\n",
    "A few outliers exist (e.g., errors above 40 or below -60), but most residuals are within a small range.\n",
    "\n",
    "‚úÖ Conclusion:\n",
    "\n",
    "Model has low bias and errors are mostly well-behaved.\n",
    "\n",
    "A few outliers might need further investigation (possibly data entry issues or rare events).\n",
    "\n",
    "### üîπ Bottom Right: Top 10 Feature Coefficients\n",
    "\n",
    "üìä Insight:\n",
    "\n",
    "Features with high positive coefficients (e.g., Coffee_Sales, Sandwich_Sales) are strong drivers of revenue.\n",
    "\n",
    "Features like Machine_Issues, Promotion_Active, and Nearby_Events have negative coefficients, meaning they reduce predicted revenue.\n",
    "\n",
    "E.g., Machine_Issues might reduce customer satisfaction ‚Üí fewer sales.\n",
    "\n",
    "Surprisingly, Promotion_Active might be ineffective or poorly timed.\n",
    "\n",
    "‚úÖ Conclusion:\n",
    "\n",
    "Business can focus on improving top features (like coffee/sandwich sales).\n",
    "\n",
    "Investigate why promotions and events are not positively impacting revenue.\n",
    "\n",
    "Reduce machine issues to avoid revenue loss.\n",
    "\n",
    "## üß† Overall Insights Summary\n",
    "\n",
    "‚úÖ High model accuracy on both training and test data (R¬≤ > 0.94)\n",
    "\n",
    "üìà Coffee and Sandwich Sales are top drivers of revenue.\n",
    "\n",
    "‚ùó Machine Issues and ineffective promotions negatively impact revenue.\n",
    "\n",
    "üìâ Residuals are random, indicating no major model bias.\n",
    "\n",
    "üìä Model is interpretable and provides actionable business recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182bdff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example predictions\n",
    "print(f\"\\nüîÆ Example Predictions:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Create sample scenarios\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Perfect Weekend Day',\n",
    "        'data': {\n",
    "            'Day_of_Week': 7, 'Is_Weekend': 1, 'Month': 6, 'Temperature_C': 25,\n",
    "            'Is_Raining': 0, 'Rainfall_mm': 0, 'Is_Holiday': 0, 'Promotion_Active': 1,\n",
    "            'Nearby_Events': 1, 'Staff_Count': 5, 'Machine_Issues': 0, 'Num_Customers': 70,\n",
    "            'Coffee_Sales': 85, 'Pastry_Sales': 45, 'Sandwich_Sales': 25,\n",
    "            'Customer_Satisfaction': 9.0, 'Day_of_Year': 150, 'Week_of_Year': 25,\n",
    "            'Quarter': 2, 'Day_Name_Encoded': 6, 'Season_Encoded': 3\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Rainy Weekday',\n",
    "        'data': {\n",
    "            'Day_of_Week': 2, 'Is_Weekend': 0, 'Month': 11, 'Temperature_C': 8,\n",
    "            'Is_Raining': 1, 'Rainfall_mm': 5.5, 'Is_Holiday': 0, 'Promotion_Active': 0,\n",
    "            'Nearby_Events': 0, 'Staff_Count': 3, 'Machine_Issues': 0, 'Num_Customers': 25,\n",
    "            'Coffee_Sales': 30, 'Pastry_Sales': 15, 'Sandwich_Sales': 8,\n",
    "            'Customer_Satisfaction': 6.5, 'Day_of_Year': 300, 'Week_of_Year': 45,\n",
    "            'Quarter': 4, 'Day_Name_Encoded': 1, 'Season_Encoded': 0\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        'name': 'Average Day',\n",
    "        'data': {\n",
    "            'Day_of_Week': 4, 'Is_Weekend': 0, 'Month': 4, 'Temperature_C': 18,\n",
    "            'Is_Raining': 0, 'Rainfall_mm': 0, 'Is_Holiday': 0, 'Promotion_Active': 0,\n",
    "            'Nearby_Events': 0, 'Staff_Count': 3, 'Machine_Issues': 0, 'Num_Customers': 45,\n",
    "            'Coffee_Sales': 50, 'Pastry_Sales': 22, 'Sandwich_Sales': 15,\n",
    "            'Customer_Satisfaction': 7.5, 'Day_of_Year': 100, 'Week_of_Year': 15,\n",
    "            'Quarter': 2, 'Day_Name_Encoded': 3, 'Season_Encoded': 2\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "for scenario in scenarios:\n",
    "    sample_df = pd.DataFrame([scenario['data']])\n",
    "    sample_scaled = scaler.transform(sample_df)\n",
    "    sample_selected = selector.transform(sample_scaled)\n",
    "    prediction = model.predict(sample_selected)[0]\n",
    "    \n",
    "    print(f\"\\n{scenario['name']}:\")\n",
    "    print(f\"  Predicted Revenue: ${prediction:.2f}\")\n",
    "    key_features = ['Num_Customers', 'Temperature_C', 'Is_Weekend', 'Promotion_Active', 'Staff_Count']\n",
    "    for feature in key_features:\n",
    "        if feature in scenario['data']:\n",
    "            print(f\"  {feature}: {scenario['data'][feature]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0bb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(f\"\\nüìä Model Summary:\")\n",
    "print(f\"‚úì Model successfully trained on {len(X_train)} samples\")\n",
    "print(f\"‚úì Test R¬≤ Score: {test_metrics['R2']:.4f}\")\n",
    "print(f\"‚úì Test RMSE: ${test_metrics['RMSE']:.2f}\")\n",
    "print(f\"‚úì Model can predict coffee shop revenue with {test_metrics['R2']*100:.1f}% accuracy\")\n",
    "print(f\"‚úì Most important factor: {feature_importance.iloc[0]['Feature']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f552ee11",
   "metadata": {},
   "source": [
    "## Save my model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba523c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the model\n",
    "joblib.dump(model, 'coffee_sales_model.pkl')\n",
    "\n",
    "# Save the scaler\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "# Save the feature selector\n",
    "joblib.dump(selector, 'feature_selector.pkl')\n",
    "\n",
    "print(\"‚úÖ Model, scaler, and selector saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
